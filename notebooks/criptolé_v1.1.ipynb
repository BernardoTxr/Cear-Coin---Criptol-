{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Versão 1.1 - Magnum\n",
    "### Estado\n",
    "1. Indicador Ecônimo - IPCA\n",
    "2. Valor do Portfólio  -> Influênciado pelas Ações\n",
    "3. Atual Alocação -> Influênciado pelas Ações\n",
    "4. Preços dos Ativos disponíveis para compra\n",
    "### Ações\n",
    "* Alocar Valor do Portfólio entre os ativos\n",
    "### Política\n",
    "* Estocástica -> Ação = Sample de Distribuição gerada pela politica:\n",
    "1.  Via Rede Neural -> Rede Gera 2 Parametros que serão usados para gerar uma distribuição gaussiana\n",
    "* Estamos usando e-greedy para controlar exploitation e exploration\n",
    "### Episódio\n",
    "* 30 dias de Alocação\n",
    "### Step\n",
    "* 1 dia de Alocar e verificar o Resultado em relação ao dia seguinte\n",
    "* Reward: Lucro\n",
    "### Treinamento\n",
    "* Percorre 30 dias calculando Recompensa Acumulada Ajustada\n",
    "* Calcula o Gradiente de Política\n",
    "* Ajusta a Rede Neural -> parametros da rede = parametros da rede + lr *  gradiente de politica\n",
    "### Para Averiguar\n",
    "* Verificar cubo de gradiente de alocação\n",
    "* Forjar série temporal que saibamos o comportamento que queremos para testar o aprendizado do modelo:\n",
    "    * Linear com ruído pra cima e pra baixo\n",
    "    * Variando Pra caceta -> Seno\n",
    "* Caso Extremo:\n",
    "    * Learning Rate e Desvio Padrão Altos -> Altissima exploração e nada de aprender\n",
    "    * Learning Rate e Desvio Padrão Baixissimo -> Não explora nada\n",
    "* Buscar Convergência fora dos vértices\n",
    "* Possíveis Dados Extras:\n",
    "    * Indicadores Econômicos:\n",
    "        * Features -> Média Móvel\n",
    "        * Risco -> Sharpe ratio, Value at Risk\n",
    "\n",
    "* Learning Rate -> Verificar Mudanças\n",
    "* Para Depois:\n",
    "    * Funções de Recompensa\n",
    "    * Evoluções de Rede Neural\n",
    "* Qual o Ideal:\n",
    "    * \n",
    "* Usar ou não usar política de exploração estilo e-greedy ou ruído faz sentido? (bernardo acredita que a maneira de modelar isso seria via desvio padrão - mas não sei como).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import torch.nn\n",
    "import os\n",
    "import mlflow\n",
    "import mlflow.pytorch\n",
    "import abc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Iniciando MlFlow\n",
    "mlflow.set_tracking_uri(\"http://127.0.0.1:5000\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Definição do Environment em que o Agente atuará\n",
    "class PortfolioEnv(gym.Env):\n",
    "    def __init__(self,arquivo_dados):\n",
    "        \"\"\"\n",
    "        Inicializa o ambiente.\n",
    "        :param dados_preco: DataFrame ou numpy array com preços dos ativos.\n",
    "                           Exemplo: colunas ['Gold', 'IBOVESPA', 'Bitcoin']\n",
    "        :param dados_observacoes: DataFrame ou numpy array com observação do modelo \n",
    "                           Exemplo: IPCA.\n",
    "        \"\"\"\n",
    "        self.arquivo_dados = arquivo_dados\n",
    "        dados = pd.read_csv(f'../data/{self.arquivo_dados}')\n",
    "        self.dados_preco = dados.iloc[:, 1:4]  # Dataframe com preços dos ativos\n",
    "        self.dados_observacao = dados.iloc[:, 4]  # Dataframe com Observações (IPCA)\n",
    "        \n",
    "        #Começamos no dia 0\n",
    "        self.dia_inicial = -1  \n",
    "        self.dia = self.dia_inicial\n",
    "\n",
    "        #Espaço de Ação e de Observação\n",
    "        self.action_space = gym.spaces.Box(low=0.0, high=1.0, shape=(3,), dtype=np.float32)\n",
    "        self.observation_space = gym.spaces.Box(low=0.0, high=np.inf, shape=(5,), dtype=np.float32)\n",
    "\n",
    "        # Estado inicial do portfólio\n",
    "        self._valor_inicial_portfolio = 100  # Valor inicial do portfólio\n",
    "         \n",
    "         #Total de Dias de um Episódio\n",
    "        self.dias_em_um_episodio = 30\n",
    "\n",
    "        #Episodio começa como não terminado\n",
    "        self._episode_ended = False\n",
    "    \n",
    "    def get_estado(self, data: int) -> np.array:\n",
    "        '''\n",
    "        Obtém o estado atual do ambiente.\n",
    "\n",
    "        :param data: Dia atual.\n",
    "        :return: Estado atual.\n",
    "        '''\n",
    "        estado = np.concatenate([\n",
    "            # alocacao,\n",
    "            # [self._valor_portfolio],\n",
    "            # self.dados_preco.iloc[self.dia],\n",
    "            [self.dados_observacao.iloc[data]]\n",
    "        ])\n",
    "        return estado\n",
    "    \n",
    "    @abc.abstractmethod\n",
    "    def calcula_recompensa(alocacao: torch.Tensor) -> torch.Tensor:\n",
    "        pass\n",
    "\n",
    "    #Função Chamada no Enviroment ao Fim de um Episódio, retorna o estado que será usado pelo o agente\n",
    "    def reset(self):\n",
    "        #Iniciado Alocação do Portfolio Aleatóriamente\n",
    "        estado_alocacao_aleatorio = np.random.uniform(0,1,3)\n",
    "        estado_aleatorio_alocacao_normalizado = estado_alocacao_aleatorio/np.sum(estado_alocacao_aleatorio)\n",
    "\n",
    "        #Passando para o dia Seguinte\n",
    "        self.dia_inicial = self.dia_inicial + 1\n",
    "        self.dia = self.dia_inicial\n",
    "\n",
    "        #Iniciado valor do portfólio para o valor inicial\n",
    "        self._valor_portfolio = self._valor_inicial_portfolio\n",
    "\n",
    "        self._episode_ended = False\n",
    "        \n",
    "        #Criando vetor de estado concatenado -> Necessário para entrada na Rede Neural\n",
    "        estado = self.get_estado(self.dia_inicial)\n",
    "        return estado\n",
    "    \n",
    "    #Função Chamada dentro de cada episódio, Recebe parametros da rede neualpara gerar uma amostra de uma distribuição normal\n",
    "    def step(self,alocacao):\n",
    "        recompensa = self.calcula_recompensa(alocacao)\n",
    "\n",
    "        #Passa para o próximo dia\n",
    "        self.dia += 1\n",
    "\n",
    "        estado = self.get_estado(self.dia)\n",
    "\n",
    "        #Alteração dentro da função step do ambiente\n",
    "        if self.dia - self.dia_inicial == 30:\n",
    "            self._episode_ended = True\n",
    "        return estado, recompensa  # Retorna o estado atual e a recompensa ao invés de chamar reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AmbienteAgenteConservador(PortfolioEnv):\n",
    "    '''\n",
    "    Implementa um ambiente de classe filha que implementa a função\n",
    "    calcula_recompensa para um agente conservador.\n",
    "    '''\n",
    "    def calcula_recompensa(self, alocacao: torch.Tensor) -> torch.Tensor:\n",
    "        '''\n",
    "        Calcula a recompensa para um agente conservador.\n",
    "\n",
    "        Args:\n",
    "            alocacao: A alocação de ativos.\n",
    "        \n",
    "        Returns:   \n",
    "            A recompensa.\n",
    "        '''\n",
    "        #Calcula a valorização do portfolio\n",
    "        valor_portfolio_pre_aplicacao = self._valor_portfolio\n",
    "        preco_usado = self.dados_preco.iloc[self.dia].values\n",
    "        preco_dia_seguinte = self.dados_preco.iloc[self.dia + 1].values\n",
    "        variacao_percentual = (preco_dia_seguinte-preco_usado)/preco_usado\n",
    "        valores_aportados = torch.multiply(alocacao,self._valor_portfolio)\n",
    "        valores_aportados_ajustados = torch.multiply(valores_aportados, torch.tensor(1 + variacao_percentual))\n",
    "        self._valor_portfolio = torch.sum(valores_aportados_ajustados)\n",
    "\n",
    "        #Calculo de Recompensa\n",
    "        recompensa = torch.subtract(self._valor_portfolio,valor_portfolio_pre_aplicacao)\n",
    "\n",
    "        #Ajustando Recompensa para aumentar grandeza de perdas\n",
    "        if recompensa < 0:\n",
    "            recompensa = recompensa*2\n",
    "\n",
    "          \n",
    "        return recompensa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AmbienteAgenteArrojado(PortfolioEnv):\n",
    "    '''\n",
    "    Implementa um ambiente de classe filha que implementa a função\n",
    "    calcula_recompensa para um agente conservador.\n",
    "    '''\n",
    "    def calcula_recompensa(self, alocacao: torch.Tensor) -> torch.Tensor:\n",
    "        '''\n",
    "        Calcula a recompensa para um agente conservador.\n",
    "\n",
    "        Args:\n",
    "            alocacao: A alocação de ativos.\n",
    "        \n",
    "        Returns:   \n",
    "            A recompensa.\n",
    "        '''\n",
    "        #Calcula a valorização do portfolio\n",
    "        valor_portfolio_pre_aplicacao = self._valor_portfolio\n",
    "        preco_usado = self.dados_preco.iloc[self.dia].values\n",
    "        preco_dia_seguinte = self.dados_preco.iloc[self.dia + 1].values\n",
    "        variacao_percentual = (preco_dia_seguinte-preco_usado)/preco_usado\n",
    "        valores_aportados = torch.multiply(alocacao,self._valor_portfolio)\n",
    "        valores_aportados_ajustados = torch.multiply(valores_aportados, torch.tensor(1 + variacao_percentual))\n",
    "        self._valor_portfolio = torch.sum(valores_aportados_ajustados)\n",
    "\n",
    "        #Calculo de Recompensa\n",
    "        recompensa = torch.subtract(self._valor_portfolio,valor_portfolio_pre_aplicacao)\n",
    "         \n",
    "        if recompensa < 0 :\n",
    "            recompensa = 0.8 * recompensa\n",
    "        elif recompensa > 0:\n",
    "            recompensa = recompensa * 1.2\n",
    "        return recompensa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Definição de Rede Neural que servirá como Política\n",
    "class PolicyNetwork(nn.Module):\n",
    "    '''\n",
    "    Observações:\n",
    "    1. Necessitamos de um modo de gerar desvio padrão positivo e diferente de 0 \n",
    "    2. Adicionar Saídas\n",
    "    '''\n",
    "    def __init__(self, state_dim, action_dim):\n",
    "        super(PolicyNetwork, self).__init__()\n",
    "        self.fc1 = nn.Linear(state_dim, 16)  # Camada oculta 1\n",
    "        self.fc1_ativ = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(16, 16)         # Camada oculta 2\n",
    "        self.fc2_ativ = nn.ReLU()\n",
    "        self.fc_mu = nn.Linear(16, action_dim)  # Média (mu) dos pesos de portfólio\n",
    "        self.output_ativ = nn.Sigmoid()\n",
    "      #  self.fc_sigma = nn.Linear(16, action_dim)  # Desvio padrão (sigma) dos pesos\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.fc1_ativ(self.fc1(x))  # Passa pela camada oculta 1\n",
    "        x = self.fc2_ativ(self.fc2(x))      # Passa pela camada oculta 2\n",
    "        mu = self.output_ativ(self.fc_mu(x))  # Média (mu), usando tanh para limitar a saída\n",
    "       # sigma = nn.functional.softplus(self.fc_sigma(x)) + 1e-6 # Desvio padrão (sigma), softplus para garantir positividade\n",
    "        return mu#, sigma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Função para treinamento do Agente\n",
    "def train(ambiente,politica,otimizador,num_episodios,fator_desconto_recompensa):\n",
    "    with mlflow.start_run():\n",
    "        resultados_treino = pd.DataFrame(columns=[\"Episodio\",\"Dia\",\"Estado\",\"Media\",\"Desvio\",\"Alocacao\",\"Log_Prob\",\"Recompensa\"])\n",
    "        for episodio in range(num_episodios):\n",
    "            #Logar Parametros no MLFLow\n",
    "            mlflow.log_params({\"Número de Episódios\":num_episodios,\n",
    "                               \"Learning Rate\":otimizador.defaults['lr'],\n",
    "                               \"Fator de Desconto\":fator_desconto_recompensa,\n",
    "                               \"Arquivo Usado\": ambiente.arquivo_dados})\n",
    "            \n",
    "            if isinstance(ambiente,AmbienteAgenteArrojado):\n",
    "                mlflow.log_param(\"Perfil do Cliente\",\"Arrojado\")\n",
    "            elif isinstance(ambiente,AmbienteAgenteConservador):\n",
    "                mlflow.log_param(\"Perfil do Cliente\",\"Conservador\")\n",
    "                \n",
    "            estado = ambiente.reset() #No inicio do episodo pegamos o estado inicial do ambiente\n",
    "            mlflow.log_param(\"Dimensão do Estado\",len(estado))\n",
    "            mlflow.log_param(\"Função de Ativação Output\",politica.output_ativ)\n",
    "            recompensas_do_episodio = [] #Array para guardar as recompensas de cada episodio\n",
    "            log_probs = [] #Array para guardar as probabilidades logaritimicas usadas no calculo do Gradiente de Politica\n",
    "            estados =[]\n",
    "            alocacoes= []\n",
    "            dias = []\n",
    "            medias = []\n",
    "            desvios = []\n",
    "        \n",
    "            while not ambiente._episode_ended:\n",
    "                #Recebendo Estado\n",
    "                estado_tensor = torch.tensor(estado, dtype=torch.float32) #Transformarmos em tensor -> Requisitado pelo Pytorch\n",
    "            # media,desvio_padrao = politica(estado_tensor) #Geramos os parametros para criação da distribuição da ação\n",
    "                media = politica(estado_tensor)\n",
    "                medias.append(media)   \n",
    "                desvio_padrao = torch.tensor([0.1,0.1,0.1])\n",
    "                desvios.append(desvio_padrao)\n",
    "            # desvios.append(desvio_padrao)\n",
    "\n",
    "                #Adicionando dia\n",
    "                dias.append(ambiente.dia)\n",
    "\n",
    "                #Gerando Alocação\n",
    "                # Função para amostrar os pesos do portfólio\n",
    "                def sample_portfolio_weights(mu, sigma):\n",
    "                    dist = torch.distributions.Normal(mu, sigma)  # Cria a distribuição normal\n",
    "                    action = dist.rsample()  # Amostragem reparametrizada\n",
    "                    log_prob = dist.log_prob(action).sum(dim=-1)  # Log-probabilidade da ação\n",
    "                    action = action/torch.sum(action)\n",
    "                    return action, log_prob\n",
    "                \n",
    "                # Amostra os pesos do portfólio\n",
    "                alocacao, log_prob = sample_portfolio_weights(media,desvio_padrao)\n",
    "                \n",
    "\n",
    "                #Guardando Estado e Alocação\n",
    "                estados.append(estado)\n",
    "                alocacoes.append(alocacao)\n",
    "\n",
    "                #Calculando log-probabilidade da ação\n",
    "                log_probs.append(log_prob)\n",
    "\n",
    "                #Executa a alocação e recebe a recompensa\n",
    "                retorno = ambiente.step(alocacao)\n",
    "                estado = retorno[0] #Atualiza o estado para o novo estado devolvido pelo ambiente e pega a recompensa da ação anterior\n",
    "                recompensa = retorno[1]\n",
    "                recompensas_do_episodio.append(recompensa)\n",
    "            #Ao final do Episódio Calcular as recompensas com desconto\n",
    "            #descontos = np.array([fator_desconto_recompensa**i for i in range(len(recompensas_do_episodio))])\n",
    "            #recompensas_com_desconto = np.array(recompensas_do_episodio) * descontos\n",
    "\n",
    "            #Calculo de Perda -> Gradiente de Politica:\n",
    "            perda = -torch.sum(torch.stack(log_probs) * torch.stack(recompensas_do_episodio))\n",
    "            otimizador.zero_grad()\n",
    "            perda.backward()\n",
    "            otimizador.step()\n",
    "            \n",
    "            # Adicionar resultados em um dataframe para análise do treinamento\n",
    "            for dia, estado, media, desvio, alocacao, log_prob, recompensa in zip(dias, estados, medias, desvios, alocacoes, log_probs, recompensas_do_episodio):\n",
    "                resultados_treino.loc[len(resultados_treino)]= {\n",
    "                    \"Episodio\": episodio,\n",
    "                    \"Dia\": dia,\n",
    "                    \"Estado\": estado,\n",
    "                    \"Media\": media.detach().numpy(),\n",
    "                    \"Desvio\": desvio.detach().numpy(),\n",
    "                    \"Alocacao\": alocacao.detach().numpy(),\n",
    "                    \"Log_Prob\": log_prob.detach().numpy(),\n",
    "                    \"Recompensa\": recompensa.detach().numpy(),\n",
    "                }\n",
    "        \n",
    "        #Salvando Treino \n",
    "        def salvartreino():\n",
    "            media_final = medias[-1].detach().numpy()\n",
    "            for i, valor in enumerate(media_final):\n",
    "                mlflow.log_metric(f\"Média Final_{i}\", valor)\n",
    "\n",
    "            # Desvio Final\n",
    "            desvio_final = desvios[-1].detach().numpy()\n",
    "            for i, valor in enumerate(desvio_final):\n",
    "                mlflow.log_metric(f\"Desvio Final_{i}\", valor)\n",
    "\n",
    "            # Alocação Final\n",
    "            alocacao_final = alocacoes[-1].detach().numpy()\n",
    "            for i, valor in enumerate(alocacao_final):\n",
    "                mlflow.log_metric(f\"Alocação Final_{i}\", valor)\n",
    "\n",
    "            #Recompensa Média por Episódio\n",
    "            mlflow.log_metric(\"Recompensa Média por Episódio\", resultados_treino.groupby('Episodio')['Recompensa'].sum().mean())\n",
    "            mlflow.log_metric(\"Dias por Episódio\", ambiente.dias_em_um_episodio)\n",
    "            max_n = 0\n",
    "            for i in os.listdir('../data/resultados_treinos/v1.1'):\n",
    "                n = i.split('_')[1]\n",
    "                n = int(n.split('.')[0])\n",
    "                if int(n) > max_n:\n",
    "                    max_n = n \n",
    "            max_n = max_n +1\n",
    "            resultados_treino.to_csv(f\"../data/resultados_treinos/v1.1/treino_{max_n}.csv\", index=False)\n",
    "             # Logar o arquivo no MLflow\n",
    "            mlflow.log_artifact(f\"../data/resultados_treinos/v1.1/treino_{max_n}.csv\")\n",
    "            mlflow.pytorch.log_model(pytorch_model=politica,artifact_path='Magnum')\n",
    "        salvartreino()\n",
    "       \n",
    "        return resultados_treino"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.11_3.11.2544.0_x64__qbz5n2kfra8p0\\Lib\\site-packages\\_distutils_hack\\__init__.py:18: UserWarning: Distutils was imported before Setuptools, but importing Setuptools also replaces the `distutils` module in `sys.modules`. This may lead to undesirable behaviors or errors. To avoid these issues, avoid using distutils directly, ensure that setuptools is installed in the traditional way (e.g. not an editable install), and/or make sure that setuptools is always imported before distutils.\n",
      "  warnings.warn(\n",
      "C:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.11_3.11.2544.0_x64__qbz5n2kfra8p0\\Lib\\site-packages\\_distutils_hack\\__init__.py:33: UserWarning: Setuptools is replacing distutils.\n",
      "  warnings.warn(\"Setuptools is replacing distutils.\")\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Episodio</th>\n",
       "      <th>Dia</th>\n",
       "      <th>Estado</th>\n",
       "      <th>Media</th>\n",
       "      <th>Desvio</th>\n",
       "      <th>Alocacao</th>\n",
       "      <th>Log_Prob</th>\n",
       "      <th>Recompensa</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.38]</td>\n",
       "      <td>[0.41775882, 0.47877842, 0.46034038]</td>\n",
       "      <td>[0.1, 0.1, 0.1]</td>\n",
       "      <td>[0.31602177, 0.37927696, 0.30470124]</td>\n",
       "      <td>3.967723</td>\n",
       "      <td>0.006936799402467386</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>[0.38]</td>\n",
       "      <td>[0.41775882, 0.47877842, 0.46034038]</td>\n",
       "      <td>[0.1, 0.1, 0.1]</td>\n",
       "      <td>[0.2504626, 0.17158228, 0.5779551]</td>\n",
       "      <td>-3.346537</td>\n",
       "      <td>0.003729934905990717</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>[0.38]</td>\n",
       "      <td>[0.41775882, 0.47877842, 0.46034038]</td>\n",
       "      <td>[0.1, 0.1, 0.1]</td>\n",
       "      <td>[0.35997513, 0.22494039, 0.4150845]</td>\n",
       "      <td>0.5674087</td>\n",
       "      <td>0.004696119786345321</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>[0.38]</td>\n",
       "      <td>[0.41775882, 0.47877842, 0.46034038]</td>\n",
       "      <td>[0.1, 0.1, 0.1]</td>\n",
       "      <td>[0.3311068, 0.30012205, 0.36877114]</td>\n",
       "      <td>3.6930184</td>\n",
       "      <td>0.005747650059902298</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>[0.38]</td>\n",
       "      <td>[0.41775882, 0.47877842, 0.46034038]</td>\n",
       "      <td>[0.1, 0.1, 0.1]</td>\n",
       "      <td>[0.2858466, 0.3592052, 0.35494816]</td>\n",
       "      <td>3.0554</td>\n",
       "      <td>0.006517263752812141</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Episodio  Dia  Estado                                 Media  \\\n",
       "0         0    0  [0.38]  [0.41775882, 0.47877842, 0.46034038]   \n",
       "1         0    1  [0.38]  [0.41775882, 0.47877842, 0.46034038]   \n",
       "2         0    2  [0.38]  [0.41775882, 0.47877842, 0.46034038]   \n",
       "3         0    3  [0.38]  [0.41775882, 0.47877842, 0.46034038]   \n",
       "4         0    4  [0.38]  [0.41775882, 0.47877842, 0.46034038]   \n",
       "\n",
       "            Desvio                              Alocacao   Log_Prob  \\\n",
       "0  [0.1, 0.1, 0.1]  [0.31602177, 0.37927696, 0.30470124]   3.967723   \n",
       "1  [0.1, 0.1, 0.1]    [0.2504626, 0.17158228, 0.5779551]  -3.346537   \n",
       "2  [0.1, 0.1, 0.1]   [0.35997513, 0.22494039, 0.4150845]  0.5674087   \n",
       "3  [0.1, 0.1, 0.1]   [0.3311068, 0.30012205, 0.36877114]  3.6930184   \n",
       "4  [0.1, 0.1, 0.1]    [0.2858466, 0.3592052, 0.35494816]     3.0554   \n",
       "\n",
       "             Recompensa  \n",
       "0  0.006936799402467386  \n",
       "1  0.003729934905990717  \n",
       "2  0.004696119786345321  \n",
       "3  0.005747650059902298  \n",
       "4  0.006517263752812141  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Treino Conservador\n",
    "ambiente = AmbienteAgenteConservador('artificial_v2.csv') #Criando Ambiente\n",
    "politica = PolicyNetwork(1, 3) #Criando Politica Estocástica\n",
    "otimizador = optim.Adam(politica.parameters(),lr=0.01) #Cria otimizador associado aos parâmetros da rede a ser atualizada\n",
    "resultados_treino = train(ambiente,politica,otimizador,num_episodios=1000,fator_desconto_recompensa=0.99)\n",
    "resultados_treino.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.11_3.11.2544.0_x64__qbz5n2kfra8p0\\Lib\\site-packages\\_distutils_hack\\__init__.py:18: UserWarning: Distutils was imported before Setuptools, but importing Setuptools also replaces the `distutils` module in `sys.modules`. This may lead to undesirable behaviors or errors. To avoid these issues, avoid using distutils directly, ensure that setuptools is installed in the traditional way (e.g. not an editable install), and/or make sure that setuptools is always imported before distutils.\n",
      "  warnings.warn(\n",
      "C:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.11_3.11.2544.0_x64__qbz5n2kfra8p0\\Lib\\site-packages\\_distutils_hack\\__init__.py:33: UserWarning: Setuptools is replacing distutils.\n",
      "  warnings.warn(\"Setuptools is replacing distutils.\")\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Episodio</th>\n",
       "      <th>Dia</th>\n",
       "      <th>Estado</th>\n",
       "      <th>Media</th>\n",
       "      <th>Desvio</th>\n",
       "      <th>Alocacao</th>\n",
       "      <th>Log_Prob</th>\n",
       "      <th>Recompensa</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.38]</td>\n",
       "      <td>[0.4906445, 0.5246462, 0.4798585]</td>\n",
       "      <td>[0.1, 0.1, 0.1]</td>\n",
       "      <td>[0.25849342, 0.3595814, 0.38192523]</td>\n",
       "      <td>3.0464144</td>\n",
       "      <td>0.00787445799468287</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>[0.38]</td>\n",
       "      <td>[0.4906445, 0.5246462, 0.4798585]</td>\n",
       "      <td>[0.1, 0.1, 0.1]</td>\n",
       "      <td>[0.26066965, 0.37453708, 0.36479324]</td>\n",
       "      <td>3.1315813</td>\n",
       "      <td>0.00812328954136774</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>[0.38]</td>\n",
       "      <td>[0.4906445, 0.5246462, 0.4798585]</td>\n",
       "      <td>[0.1, 0.1, 0.1]</td>\n",
       "      <td>[0.33770424, 0.36364052, 0.29865527]</td>\n",
       "      <td>2.2567682</td>\n",
       "      <td>0.008068563349576152</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>[0.38]</td>\n",
       "      <td>[0.4906445, 0.5246462, 0.4798585]</td>\n",
       "      <td>[0.1, 0.1, 0.1]</td>\n",
       "      <td>[0.3659752, 0.3340899, 0.29993483]</td>\n",
       "      <td>3.8655562</td>\n",
       "      <td>0.007566955743385506</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>[0.38]</td>\n",
       "      <td>[0.4906445, 0.5246462, 0.4798585]</td>\n",
       "      <td>[0.1, 0.1, 0.1]</td>\n",
       "      <td>[0.2884731, 0.43184116, 0.2796857]</td>\n",
       "      <td>3.024898</td>\n",
       "      <td>0.009118035606718422</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Episodio  Dia  Estado                              Media           Desvio  \\\n",
       "0         0    0  [0.38]  [0.4906445, 0.5246462, 0.4798585]  [0.1, 0.1, 0.1]   \n",
       "1         0    1  [0.38]  [0.4906445, 0.5246462, 0.4798585]  [0.1, 0.1, 0.1]   \n",
       "2         0    2  [0.38]  [0.4906445, 0.5246462, 0.4798585]  [0.1, 0.1, 0.1]   \n",
       "3         0    3  [0.38]  [0.4906445, 0.5246462, 0.4798585]  [0.1, 0.1, 0.1]   \n",
       "4         0    4  [0.38]  [0.4906445, 0.5246462, 0.4798585]  [0.1, 0.1, 0.1]   \n",
       "\n",
       "                               Alocacao   Log_Prob            Recompensa  \n",
       "0   [0.25849342, 0.3595814, 0.38192523]  3.0464144   0.00787445799468287  \n",
       "1  [0.26066965, 0.37453708, 0.36479324]  3.1315813   0.00812328954136774  \n",
       "2  [0.33770424, 0.36364052, 0.29865527]  2.2567682  0.008068563349576152  \n",
       "3    [0.3659752, 0.3340899, 0.29993483]  3.8655562  0.007566955743385506  \n",
       "4    [0.2884731, 0.43184116, 0.2796857]   3.024898  0.009118035606718422  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Treino Arrojado\n",
    "ambiente = AmbienteAgenteArrojado('artificial_v2.csv') #Criando Ambiente\n",
    "politica = PolicyNetwork(1, 3) #Criando Politica Estocástica\n",
    "otimizador = optim.Adam(politica.parameters(),lr=0.1) #Cria otimizador associado aos parâmetros da rede a ser atualizada\n",
    "resultados_treino = train(ambiente,politica,otimizador,num_episodios=1000,fator_desconto_recompensa=0.99)\n",
    "resultados_treino.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
